{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd=os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, cwd)\n",
    "import rasterio\n",
    "import folium\n",
    "import branca\n",
    "from folium.plugins import HeatMap\n",
    "import re\n",
    "import branca.colormap as cmp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapefile as shp\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import utils.processing as pr\n",
    "import utils.s3_utils as s3\n",
    "import utils.plot as pl\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from folium import plugins\n",
    "import config.paths as path\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "import utils.modeling as ml\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "region_code = 0\n",
    "modelname = path.model_path\n",
    "\n",
    "trainingdata_path = f'../../data/{modelname}/training/final/clustered/region_{region_code}/'\n",
    "save_path=f'../../model/log-models/final-log-model-3/region-{region_code}/'\n",
    "\n",
    "drop_features = 'y'\n",
    "test_train = 'y'\n",
    "test_split = .2\n",
    "eval_method = 'f1_weighted'\n",
    "\n",
    "# Import data\n",
    "x = pd.read_csv(trainingdata_path + 'training_x/x.csv')\n",
    "x = x.drop(['Unnamed: 0'],axis=1)\n",
    "y = pd.read_csv(trainingdata_path + 'training_y/y.csv')\n",
    "y = y.drop(['Unnamed: 0'],axis=1)\n",
    "dummies_train = pd.read_csv(trainingdata_path + 'training_x/dummies.csv')\n",
    "dummies_train = dummies_train.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "\n",
    "#Remove pearson correlated features, while keeping the highest coeff feature in orig. model\n",
    "if drop_features == 'y':\n",
    "    drop_cols = path.logreg_col_drop_dict[region_code]\n",
    "    x = x.drop(drop_cols, axis=1)\n",
    "    save_path=save_path+'feature-selection/'\n",
    "    Path(f\"{save_path}\").mkdir(parents=True, exist_ok=True)\n",
    "else: \n",
    "    save_path=save_path\n",
    "    Path(f\"{save_path}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "\n",
    "nuts3_list = list(y.nuts_id.unique()) #will drop by 20% each run\n",
    "nuts3_total_districts = len(nuts3_list)\n",
    "random.seed(11)\n",
    "drop_descriptor_cols=['nuts_id','nuts_name','longitude','latitude']\n",
    "y_pred_list = []\n",
    "if test_train == 'y':\n",
    "    ### This section only runs if there is a saved clf model with best_params configured (produced in gridsearch section below)\n",
    "    ### This will error out if you do not have saved clf model params. Please set test_train config to 'y' once params are set. \n",
    "    for i in range(0,int(1/test_split)):\n",
    "        #randomly select test_split % of all nuts_ids, drop them from nuts 3 set space for next run\n",
    "        #if-then clause makes sure we get the extra remaining nuts3 district from split\n",
    "        if i == max(range(0,int(1/test_split))):\n",
    "            test_nuts3_districts = nuts3_list\n",
    "        else:\n",
    "            test_nuts3_districts = random.sample(nuts3_list, int(nuts3_total_districts * test_split))\n",
    "        nuts3_list = [x for x in nuts3_list if x not in list(test_nuts3_districts)]\n",
    "        print(f'***NUTS3 IDs sampled for Run {i}: {test_nuts3_districts}***')\n",
    "        print(nuts3_list)\n",
    "\n",
    "        y_train = y[~y['nuts_id'].isin(test_nuts3_districts)]\n",
    "        x_train = x[~x['nuts_id'].isin(test_nuts3_districts)]\n",
    "\n",
    "        y_test = y[y['nuts_id'].isin(test_nuts3_districts)]\n",
    "        x_test = x[x['nuts_id'].isin(test_nuts3_districts)]\n",
    "\n",
    "        y_train = y_train.drop(drop_descriptor_cols, axis=1)\n",
    "        x_train = x_train.drop(drop_descriptor_cols, axis=1)\n",
    "\n",
    "        y_test = y_test.drop(['nuts_id','nuts_name'], axis=1)\n",
    "        x_test = x_test.drop(drop_descriptor_cols, axis=1)\n",
    "         \n",
    "        ## Scale x_train values\n",
    "        lin_sc = StandardScaler()\n",
    "        lin_sc.fit(x_train)\n",
    "        x_scaled = lin_sc.transform(x_train)\n",
    "\n",
    "        #model\n",
    "        lasso, importances = ml.logreg_model(x_scaled,y_train, x=x_test)\n",
    "\n",
    "        ### get roc-auc score by predicted using fitted model\n",
    "        x_scaled = lin_sc.transform(x_test)\n",
    "        y_test['pred'] = lasso.predict_proba(x_scaled)[:, 1]\n",
    "        auroc_score = roc_auc_score(y_test['presence'], y_test['pred'])\n",
    "        y_pred_list.append(y_test)\n",
    "        print(f'AUROC for Test Run {i}:{auroc_score}')\n",
    "        print(f'*** END OF RUN {i} ***')\n",
    "        print('')\n",
    "\n",
    "    print('***Cross-Validation Complete***\\n')\n",
    "    y_pred = pd.concat(y_pred_list)\n",
    "    y_pred = pr.remove_or_combine_duplicates(y_pred, strategy='aggregate', aggfunc='mean')\n",
    "\n",
    "    auroc_score = roc_auc_score(y_pred['presence'], y_pred['pred'])\n",
    "    print(f'***AUROC for Cross-Validation:{auroc_score}***')\n",
    "\n",
    "    print('***Dropping descriptor columns from x and y dfs***')\n",
    "    x = x.drop(drop_descriptor_cols, axis=1)\n",
    "    y = y.drop(drop_descriptor_cols, axis=1)\n",
    "    param_text = str(f'''\n",
    "                     *** Region {region_code} Logistic Regression Results Summary\n",
    "                     train-test split: {test_split}\n",
    "                     ROC-AUC score: {auroc_score}\n",
    "                     features dropped: {drop_features}\n",
    "                     dropped cols: {drop_cols}\n",
    "                     ''')\n",
    "    with open(save_path+'roc-auc-test.txt', \"w\") as text_file:\n",
    "        text_file.write(param_text)\n",
    "\n",
    "if test_train == 'n':\n",
    "    x = x.drop(drop_descriptor_cols, axis=1)\n",
    "    y = y.drop(drop_descriptor_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions on full Cov Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_sc = StandardScaler()\n",
    "lin_sc.fit(x)\n",
    "x_scaled = lin_sc.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in-sample testing\n",
    "lasso, importances = ml.logreg_model(x_scaled,y,x)\n",
    "y_pred = (lasso.predict_proba(x_scaled)[:,1]).round(3)\n",
    "\n",
    "auroc_score = roc_auc_score(y['presence'], y_pred)\n",
    "print(f'***AUROC for Cross-Validation:{auroc_score}***')\n",
    "\n",
    "param_text = f'''\n",
    "*** Region {region_code} Logistic Regression Results Summary\n",
    "train-test split: {test_split}\n",
    "ROC-AUC score: {auroc_score}\n",
    "features dropped: {drop_features}\n",
    "dropped cols: {drop_cols}\n",
    "                    '''\n",
    "with open(save_path+'roc-auc-insample.txt', \"w\") as text_file:\n",
    "    text_file.write(param_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view and save feature importance results\n",
    "display(importances.head(10))\n",
    "display(importances.tail(10))\n",
    "save_name=f\"feature_importances.csv\"\n",
    "importances.to_csv(save_path+save_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Predictions\n",
    "#get predictors for all countries\n",
    "test_list = []\n",
    "sample_sizer = 5\n",
    "fn = f'../../data/{modelname}/processed-predictor-parquets/clustered/{region_code}-predictors.parquet'\n",
    "df = pd.read_parquet(fn, engine='pyarrow')\n",
    "samp_size = df.shape[0]/sample_sizer\n",
    "print(f'sample_size of {region_code} : {samp_size}')\n",
    "\n",
    "df = df.sample(int(np.round(samp_size)), random_state=42)\n",
    "test_list.append(df)\n",
    "\n",
    "test = pd.concat(test_list)\n",
    "\n",
    "del df, test_list\n",
    "\n",
    "\n",
    "test_df = test.drop(['geometry','landcover'],axis=1)\n",
    "test_df.columns = map(str.lower, test_df.columns)\n",
    "\n",
    "#get dummies for landcover \n",
    "\n",
    "dummies_test = pd.get_dummies(test_df['cat'])\n",
    "#add dummy categoricals to match dummies_train used in training\n",
    "for missing_env in set(dummies_train.columns).difference(set(dummies_test.columns)):\n",
    "    dummies_test[missing_env] = 0\n",
    "\n",
    "print('finished splitting dummy variables')\n",
    "\n",
    "test_df2 = test_df.drop('cat',axis=1)\n",
    "test_df2 = pd.concat([test_df2, dummies_test], axis = 1).reset_index().drop('index',axis=1)\n",
    "\n",
    "#Match and Re-order columns to the same as was used in training dataframe x\n",
    "x_test = x.rename(columns = {'tg-grp-mean-days-above-5degC-monthly-ratio':'tg-grp-mean-days-above-5degc-monthly-ratio'})\n",
    "nolatlon = test_df2[x_test.columns]\n",
    "\n",
    "x_test = nolatlon\n",
    "x_test = x_test.rename(columns = {'tg-grp-mean-days-above-5degc-monthly-ratio':'tg-grp-mean-days-above-5degC-monthly-ratio'})\n",
    "\n",
    "\n",
    "###\n",
    "### To prevent leakage, scale values according to training scaler from above\n",
    "print('Scaling values')\n",
    "#scale values\n",
    "test_scaled = lin_sc.transform(x_test)\n",
    "\n",
    "print('Running prediction')\n",
    "test_output = pd.DataFrame()\n",
    "test_output['pred'] = (lasso.predict_proba(test_scaled)[:,1]).round(3)\n",
    "test_output['latitude'] = test_df2['lat_env']\n",
    "test_output['longitude'] = test_df2['lon_env']\n",
    "\n",
    "print('Outputting prediction data')\n",
    "print('removing duplicate columns')\n",
    "test_output = pr.remove_or_combine_duplicates(test_output, strategy='aggregate', aggfunc='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output.to_csv(save_path+f'test-predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "histo = test_output.pred.hist(bins=100)\n",
    "fig = histo.figure\n",
    "fig.savefig(save_path+'distribution_histogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predictions on all of eu - can cut out later with shapefile. \n",
    "country_codes = ['DK','NO','SE','FI','AT','CH','CZ','DE','EE','FR', 'LT','LV','NL','PL','SK', 'IT','UK','HR','BE','SI','LU']\n",
    "m, ma = pl.plot_log_maps(test_output, country_codes,region=region_code, dot_sample=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name=f\"logreg-dotmap.html\"\n",
    "m.save(save_path+save_name)\n",
    "\n",
    "save_name=f\"logreg-choropleth.html\"\n",
    "ma.save(save_path+save_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
