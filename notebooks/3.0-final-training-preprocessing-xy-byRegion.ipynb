{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd=os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, cwd)\n",
    "import rasterio\n",
    "import folium\n",
    "import branca\n",
    "from folium.plugins import HeatMap\n",
    "import re\n",
    "import branca.colormap as cmp\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapefile as shp\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import utils.processing as pr\n",
    "import utils.s3_utils as s3\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from folium import plugins\n",
    "import config.paths as path\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from pathlib import Path\n",
    "\n",
    "region_code_list = [0,1,2]\n",
    "modelname = path.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for region_code in region_code_list: \n",
    "    #get regional training data\n",
    "    trainingdata_path = f'../../data/{modelname}/training/final/clustered/region_{region_code}/'\n",
    "    fn = trainingdata_path + f'training_data/{region_code}-training-data.csv'\n",
    "    df_foci = pd.read_csv(fn).reset_index(drop=True)\n",
    "    df_foci = df_foci.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "    # Renaming columns\n",
    "    df_foci  = df_foci.rename(columns={'longitude_left': 'longitude', 'latitude_left': 'latitude'})\n",
    "    df_foci.columns = df_foci.columns.str.lower()\n",
    "    df_foci = df_foci.rename({\n",
    "                'tg-grp-mean-days-above-5degc-monthly-ratio':'tg-grp-mean-days-above-5degC-monthly-ratio', \n",
    "                }, axis=1)\n",
    "    display(df_foci.shape)\n",
    "    df1 = df_foci\n",
    "\n",
    "    # get the covariate values and labels given in config.paths to be consistent with maxent 2.4 processing\n",
    "    # also filter by the columns given in the config.paths\n",
    "    cov_list = list(path.cov_dict.keys())[1:]\n",
    "    cov_list.append('nuts_id')\n",
    "    cov_list.append('nuts_name')\n",
    "    cov_list.append('latitude')\n",
    "    cov_list.append('longitude')\n",
    "    df1 = df1[cov_list]\n",
    "\n",
    "    df1['presence'] = df_foci['presence']\n",
    "    df1['cat'] = df_foci['cat']\n",
    "    inv_cov_dict = {v: k for k, v in path.cov_dict.items()}\n",
    "    df1 = df1.rename(columns = inv_cov_dict)\n",
    "\n",
    "    df1_presence = df1[(df1.presence == 1)]\n",
    "    display(df1.shape)\n",
    "\n",
    "    print('Exporting x and y foci-only data... ')\n",
    "    df1.to_csv(trainingdata_path + 'training-foci-only/x.csv')\n",
    "\n",
    "    #append pseudoabsence data\n",
    "    #get regional pseudoabsence data\n",
    "    fn = trainingdata_path + f'pseudoabsence_data/{region_code}-pseudoabsence-data.csv'\n",
    "    df_pseudoabsence = pd.read_csv(fn).reset_index(drop=True)\n",
    "    df_pseudoabsence = df_pseudoabsence.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # get the covariate values and labels given in config.paths to be consistent with maxent 2.4 processing\n",
    "    # also filter by the columns given in the config.paths\n",
    "    # also include nuts_id, nuts_name\n",
    "    cov_list = list(path.cov_dict.keys())[1:]\n",
    "    cov_list.append('nuts_id')\n",
    "    cov_list.append('nuts_name')\n",
    "    cov_list.append('lon_env')\n",
    "    cov_list.append('lat_env')\n",
    "    pseudo_df = df_pseudoabsence[cov_list]\n",
    "\n",
    "    pseudo_df  = pseudo_df.rename(columns={'lon_env': 'longitude', 'lat_env': 'latitude'})\n",
    "\n",
    "\n",
    "    pseudo_df['presence'] = df_pseudoabsence['presence']\n",
    "    pseudo_df['cat'] = df_pseudoabsence['cat']\n",
    "    inv_cov_dict = {v: k for k, v in path.cov_dict.items()}\n",
    "    pseudo_df = pseudo_df.rename(columns = inv_cov_dict)\n",
    "\n",
    "\n",
    "    #drop unnecessary columns\n",
    "    x_train = df1_presence.drop_duplicates()\n",
    "\n",
    "    # Pseudoabsence filtered modeling\n",
    "    print(f'Check to see if pseudoabsence covariates line up with training_dataset: {set(pseudo_df.columns).difference(set(x_train.columns))}')\n",
    "\n",
    "    x_train = x_train[x_train.columns.sort_values()]\n",
    "    pseudo_df = pseudo_df[pseudo_df.columns.sort_values()]\n",
    "\n",
    "    #concatenate tbev data with pseudoabsence points\n",
    "    x_train = pd.concat([x_train,pseudo_df])\n",
    "\n",
    "    # #get dummies for landcover \n",
    "    dummies_train = pd.get_dummies(x_train[['cat']],dummy_na=True).drop('cat_nan',axis=1)\n",
    "    x_train = x_train.drop(['cat','landcover'],axis=1)\n",
    "\n",
    "    x_train = pd.concat([x_train,dummies_train],axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    y = x_train[['presence','nuts_id','nuts_name','latitude','longitude']]\n",
    "    x = x_train.drop(['presence'],axis=1)\n",
    "\n",
    "    print('Exporting x and y data as csvs... ')\n",
    "    x.to_csv(trainingdata_path + 'training_x/x.csv')\n",
    "    y.to_csv(trainingdata_path + 'training_y/y.csv')\n",
    "    dummies_train.to_csv(trainingdata_path + 'training_x/dummies.csv')\n",
    "\n",
    "    import scipy\n",
    "    import scipy\n",
    "    import pandas as pd\n",
    "    region = 0\n",
    "\n",
    "    p_val = .01\n",
    "    corr_val=.95\n",
    "\n",
    "    landcover_cols = [col for col in x.columns if 'cat_' in col]\n",
    "    X = x.drop(['nuts_id','nuts_name','latitude','longitude'],axis=1)\n",
    "    X = X.drop(landcover_cols,axis=1)\n",
    "\n",
    "    #Calculate Pearson correlation coefficient and p-value\n",
    "    cov_pair_list = []\n",
    "    pos_covs=[]\n",
    "    corr_var_text = f'*** Pearson Correlation Coef. Related Variables (p<{p_val}, corr>{corr_val}) ***'\n",
    "    for cov1 in X.columns:\n",
    "        for cov2 in X.columns:\n",
    "            if cov1==cov2: #excludes self-correlating the same cov\n",
    "                pass\n",
    "            elif (cov1, cov2) in cov_pair_list: #avoids duplicate correlations (cov1,cov2) == (cov2,cov1)\n",
    "                pass\n",
    "            else:\n",
    "                cov_pair_list.append((cov1,cov2))\n",
    "                corr, p_value = scipy.stats.pearsonr(X[cov1], X[cov2])\n",
    "\n",
    "                # Interpretation of the correlation\n",
    "                if (p_value < p_val) & (corr > corr_val):\n",
    "                    print(f\"Results for {cov1}, {cov2}\")\n",
    "                    print(f\"Pearson Correlation Coefficient: {corr}\")\n",
    "                    print(f\"P-value: {p_value:.9f}\")\n",
    "                    print ('***********************************')\n",
    "                    corr_var_text = ''.join((corr_var_text,'\\n' + f'{cov1} and {cov2} are related',''))\n",
    "                    pos_covs.append((cov2,cov1))\n",
    "                else:\n",
    "                    pass\n",
    "            cov_pair_list.append((cov2,cov1))\n",
    "    print(corr_var_text)\n",
    "    r_ptest_corr_df = pd.DataFrame({'var1':[x[0] for x in pos_covs], 'var2':[x[1] for x in pos_covs]})\n",
    "    r_ptest_corr_df.to_csv(trainingdata_path + \"stats_tests/pearson-ptest-correlation-results.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output foci only data incl absences\n",
    "df1.to_csv(trainingdata_path + 'training-foci-only/foci.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append pseudoabsence data\n",
    "#get regional pseudoabsence data\n",
    "fn = trainingdata_path + f'pseudoabsence_data/{region_code}-pseudoabsence-data.csv'\n",
    "df_pseudoabsence = pd.read_csv(fn).reset_index(drop=True)\n",
    "df_pseudoabsence = df_pseudoabsence.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "\n",
    "# get the covariate values and labels given in config.paths to be consistent with maxent 2.4 processing\n",
    "# also filter by the columns given in the config.paths\n",
    "# also include nuts_id, nuts_name\n",
    "cov_list = list(path.cov_dict.keys())[1:]\n",
    "cov_list.append('nuts_id')\n",
    "cov_list.append('nuts_name')\n",
    "cov_list.append('lon_env')\n",
    "cov_list.append('lat_env')\n",
    "pseudo_df = df_pseudoabsence[cov_list]\n",
    "\n",
    "pseudo_df  = pseudo_df.rename(columns={'lon_env': 'longitude', 'lat_env': 'latitude'})\n",
    "\n",
    "\n",
    "pseudo_df['presence'] = df_pseudoabsence['presence']\n",
    "pseudo_df['cat'] = df_pseudoabsence['cat']\n",
    "inv_cov_dict = {v: k for k, v in path.cov_dict.items()}\n",
    "pseudo_df = pseudo_df.rename(columns = inv_cov_dict)\n",
    "\n",
    "print(pseudo_df.columns)\n",
    "print(df1_presence.columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns\n",
    "x_train = df1_presence.drop_duplicates()\n",
    "\n",
    "# Pseudoabsence filtered modeling\n",
    "print(f'Check to see if pseudoabsence covariates line up with training_dataset: {set(pseudo_df.columns).difference(set(x_train.columns))}')\n",
    "\n",
    "x_train = x_train[x_train.columns.sort_values()]\n",
    "pseudo_df = pseudo_df[pseudo_df.columns.sort_values()]\n",
    "\n",
    "#concatenate tbev data with pseudoabsence points\n",
    "x_train = pd.concat([x_train,pseudo_df])\n",
    "\n",
    "# #get dummies for landcover \n",
    "dummies_train = pd.get_dummies(x_train[['cat']],dummy_na=True).drop('cat_nan',axis=1)\n",
    "x_train = x_train.drop(['cat','landcover'],axis=1)\n",
    "\n",
    "x_train = pd.concat([x_train,dummies_train],axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "y = x_train[['presence','nuts_id','nuts_name','latitude','longitude']]\n",
    "x = x_train.drop(['presence'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv(trainingdata_path + 'training_x/x.csv')\n",
    "y.to_csv(trainingdata_path + 'training_y/y.csv')\n",
    "dummies_train.to_csv(trainingdata_path + 'training_x/dummies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection - Pearson Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract related variables based on a threshold\n",
    "def extract_related_variables(correlation_matrix_df, threshold=0.7):\n",
    "    related_variables = []\n",
    "    n = correlation_matrix_df.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if abs(correlation_matrix_df.iloc[i, j]) >= threshold:\n",
    "                related_variables.append((correlation_matrix_df.columns[i], correlation_matrix_df.columns[j]))\n",
    "    return related_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get pearson correlation matrix using x but drop nuts district cols.\n",
    "corr_df = x.drop(['nuts_id','nuts_name','latitude','longitude'],axis=1).corr(method='pearson')\n",
    "corr_df.to_csv(trainingdata_path+'stats_tests/pearson_matrix.csv')\n",
    "\n",
    "# Set your desired threshold\n",
    "pearson_threshold=.90\n",
    "\n",
    "corr_df = corr_df[corr_df!=1]\n",
    "\n",
    "# Extract related variables\n",
    "related_variables = extract_related_variables(corr_df, pearson_threshold)\n",
    "\n",
    "# Print the related variables\n",
    "corr_var_text = '*** Pearson Correlation Coef. Related Variables ***'\n",
    "for var1, var2 in related_variables:\n",
    "    corr_var_text = ''.join((corr_var_text,'\\n' + f'{var1} and {var2} are related',''))\n",
    "\n",
    "print(corr_var_text)\n",
    "\n",
    "# Save related variables as text file\n",
    "with open(trainingdata_path+\"stats_tests/pearson-correlation-results.txt\", \"w\") as text_file:\n",
    "   text_file.write(corr_var_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scipy (deprecated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy\n",
    "import pandas as pd\n",
    "region = 0\n",
    "\n",
    "p_val = .01\n",
    "corr_val=.95\n",
    "\n",
    "landcover_cols = [col for col in x.columns if 'cat_' in col]\n",
    "X = x.drop(['nuts_id','nuts_name','latitude','longitude'],axis=1)\n",
    "X = X.drop(landcover_cols,axis=1)\n",
    "\n",
    "#Calculate Pearson correlation coefficient and p-value\n",
    "cov_pair_list = []\n",
    "pos_covs=[]\n",
    "corr_var_text = f'*** Pearson Correlation Coef. Related Variables (p<{p_val}, corr>{corr_val}) ***'\n",
    "for cov1 in X.columns:\n",
    "    for cov2 in X.columns:\n",
    "        if cov1==cov2: #excludes self-correlating the same cov\n",
    "            pass\n",
    "        elif (cov1, cov2) in cov_pair_list: #avoids duplicate correlations (cov1,cov2) == (cov2,cov1)\n",
    "            pass\n",
    "        else:\n",
    "            cov_pair_list.append((cov1,cov2))\n",
    "            corr, p_value = scipy.stats.pearsonr(X[cov1], X[cov2])\n",
    "\n",
    "            # Interpretation of the correlation\n",
    "            if (p_value < p_val) & (corr > corr_val):\n",
    "                print(f\"Results for {cov1}, {cov2}\")\n",
    "                print(f\"Pearson Correlation Coefficient: {corr}\")\n",
    "                print(f\"P-value: {p_value:.9f}\")\n",
    "                print ('***********************************')\n",
    "                corr_var_text = ''.join((corr_var_text,'\\n' + f'{cov1} and {cov2} are related',''))\n",
    "                pos_covs.append((cov2,cov1))\n",
    "            else:\n",
    "                pass\n",
    "        cov_pair_list.append((cov2,cov1))\n",
    "print(corr_var_text)\n",
    "r_ptest_corr_df = pd.DataFrame({'var1':[x[0] for x in pos_covs], 'var2':[x[1] for x in pos_covs]})\n",
    "r_ptest_corr_df.to_csv(trainingdata_path + \"stats_tests/pearson-ptest-correlation-results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ptest_corr_df = pd.DataFrame({'var1':[x[0] for x in pos_covs], 'var2':[x[1] for x in pos_covs]})\n",
    "r_ptest_corr_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
