{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd=os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, cwd)\n",
    "import utils.s3_utils as s3\n",
    "import pandas as pd\n",
    "import shapefile as shp\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import folium\n",
    "import utils.processing as pr\n",
    "import os \n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "from pyproj import Geod\n",
    "import geopandas as gpd\n",
    "import config.paths as path\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list = [0,1,2]\n",
    "modelname = path.model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section combines predictor data by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab environment mapping variables\n",
    "env_db_path = '../../data/raw-data/landcover-EEA/eea_r_3035_100_m_ecosystem-types-terrestrial-c_p_2012_v03_r01/mapping-dict-eea_r_3035_100_m_etm-terrestrial-c_2012_v3-1_r00.tif.vat.csv'\n",
    "env_db = pd.read_csv(env_db_path).rename({'Value,N,10,0':'env_code','EUNIS_L2,C,254':'env_type'},axis=1)\n",
    "env_map = dict(zip(env_db.env_code,env_db.env_type))\n",
    "env_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Newer method - sjoins each weather data in piecemeal so we don't have missing data from outer joining all climate variables\n",
    "### Loop through country list to merge landcover, elevation, reservoir, weather data (in order)\n",
    "for region_code in region_list:\n",
    "    print(f'Beginning to process {region_code} predictors')\n",
    "\n",
    "    ### Folding in Raster data ###\n",
    "    print(f'Pulling in raster data for {region_code}')\n",
    "\n",
    "    fp = f'../../data/{modelname}/raster-combined/eea-.1degclim-deer-reservoir-processed/clustered/raster_region_{region_code}.tif' #local\n",
    "    ds = rxr.open_rasterio(fp, band_as_variable=True)\n",
    "\n",
    "    env = ds.to_dataframe().reset_index()\n",
    "    env = env.rename({\n",
    "        'x':'lon_env', \n",
    "        'y':'lat_env', \n",
    "        'band_1':'Apodemus_flavicollis',\n",
    "        'band_2':'Apodemus_sylvaticus',\n",
    "        'band_3':'Cervus_elaphus',\n",
    "        'band_4':'Dama_dama',\n",
    "        'band_5':'Microtus_subterraneus',\n",
    "        'band_6':'Myodes_glareolus',\n",
    "        'band_7':'landcover',\n",
    "        'band_8':'elev',\n",
    "        'band_9':'red_deer',\n",
    "        'band_10':'roe_deer'\n",
    "        }, axis=1) \n",
    "    env = env.drop(['spatial_ref'],axis=1)\n",
    "    del ds\n",
    "    print(f'Shape of {region_code} env raster df before trimming: {env.shape}')\n",
    "\n",
    "    #remove -9999 (NODATA from raster proc) from landcover\n",
    "    env = env[env.landcover !=-9999]\n",
    "\n",
    "    env['cat'] = [env_map[int(i)] for i in env['landcover']]\n",
    "    \n",
    "    #remove elevation NODATA values\n",
    "    env=env[env['elev']!=-9999]\n",
    "\n",
    "    \n",
    "    #set red_deer and roe_deer nodata variables to 0. \n",
    "    env.loc[env['red_deer'] == -9999, 'red_deer'] = 0\n",
    "    env.loc[env['roe_deer'] == -9999, 'roe_deer'] = 0\n",
    "\n",
    "    # #set agustin reservoir nodata variables to 0\n",
    "    env.loc[env['Apodemus_flavicollis'] == -9999, 'Apodemus_flavicollis'] = 0\n",
    "    env.loc[env['Apodemus_sylvaticus'] == -9999, 'Apodemus_sylvaticus'] = 0\n",
    "    env.loc[env['Cervus_elaphus'] == -9999, 'Cervus_elaphus'] = 0\n",
    "    env.loc[env['Dama_dama'] == -9999, 'Dama_dama'] = 0\n",
    "    env.loc[env['Microtus_subterraneus'] == -9999, 'Microtus_subterraneus'] = 0\n",
    "    env.loc[env['Myodes_glareolus'] == -9999, 'Myodes_glareolus'] = 0\n",
    "\n",
    "    #create env_gdf\n",
    "    env_gdf = gpd.GeoDataFrame(\n",
    "        env, geometry=gpd.points_from_xy(env['lon_env'], env['lat_env']))\n",
    "\n",
    "    print(f'Shape of {region_code} env raster df after trimming and creating gdf from df: {env.shape}')\n",
    "    del env\n",
    "\n",
    "    #get env range to clip weather+res data, plus some buffer to include nearest weather stations outside of borders\n",
    "    lat_min = env_gdf['lat_env'].min()\n",
    "    lon_min = env_gdf['lon_env'].min()\n",
    "    lat_max = env_gdf['lat_env'].max()\n",
    "    lon_max = env_gdf['lon_env'].max()\n",
    "\n",
    "    env_lat_range = [lat_min-.5, lat_max+.5]\n",
    "    env_lon_range = [lon_min-.5, lon_max+.5]\n",
    "    print(f'weather clipping range from elevation/landcover is: {env_lat_range}, {env_lon_range}')\n",
    "\n",
    "    ### Use weather data from earlier and merge in\n",
    "\n",
    "    ### Grab weather data here once\n",
    "    print('Merging in weather data')\n",
    "    fp = f'../../data/{modelname}/feature_engineering/0.10deg/'\n",
    "    feature_df_list = sorted([f for f in os.listdir(fp) if not f.startswith('.')])\n",
    "    merge_ls = []\n",
    "\n",
    "    final_pred = env_gdf.copy()\n",
    "    for fn in feature_df_list:\n",
    "        #pull in min temp data from local\n",
    "        filename = fp+fn\n",
    "        print(f'Merging in {filename}')\n",
    "        main_df = pd.read_parquet(filename, engine='pyarrow') # filter clause, filter = [(\"tn\", \">\", 12)])\n",
    "        \n",
    "        #clip weather data from outside country\n",
    "\n",
    "        # #next section is for any aggregations done by collection group (ie 2000-2022 windSpeed)\n",
    "        col_var = main_df.columns[3]\n",
    "        p = fn[0:3]\n",
    "        aggtype = 'grp-mean-'\n",
    "        df = main_df.groupby([f'longitude',f'latitude']).agg({\n",
    "            col_var : 'mean'\n",
    "            }).reset_index()\n",
    "        df.columns = ['longitude','latitude', p+aggtype+col_var]\n",
    "\n",
    "        cntry_clim = df[((df['latitude'] >= env_lat_range[0]) & (df['latitude'] <= env_lat_range[1])) & ((df['longitude'] >= env_lon_range[0]) & (df['longitude'] <= env_lon_range[1]))]\n",
    "\n",
    "        clim_gdf = gpd.GeoDataFrame(\n",
    "            cntry_clim,geometry=gpd.points_from_xy(cntry_clim['longitude'], cntry_clim['latitude']))\n",
    "\n",
    "        # join country climate gdf to env raster variables\n",
    "\n",
    "        final_pred = final_pred.sjoin_nearest(clim_gdf,exclusive=True)\n",
    "        final_pred = final_pred.drop(['index_right','latitude','longitude'],axis=1)\n",
    "\n",
    "        del cntry_clim\n",
    "    \n",
    "    print('joining nuts 3 cntr_code and nuts_id/name')\n",
    "    final_pred = pr.nuts_join_regions(final_pred)\n",
    "    print('done joining nuts3')\n",
    "\n",
    "    print(f'Shape of {region_code} finalpred df after adding climate data: {final_pred.shape}')\n",
    "\n",
    "    print('writing parquet file to directory')\n",
    "    write_filename = f'../../data/{modelname}/processed-predictor-parquets/clustered/{region_code}-predictors.parquet'\n",
    "    final_pred.to_parquet(write_filename, compression ='snappy')\n",
    "    print(f'{region_code} predictors complete')\n",
    "    del final_pred\n",
    "    print('final_pred df deleted')\n",
    "    print('***********************************************')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tick Training Data (sample with data) + Pseudabsence Processing Data\n",
    "\n",
    "Combine covariates and tick foci data into training data set for sklearn models.\n",
    "Combine covariates and random pseudabsence points into pseudabsence training data set for sklearn models.\n",
    "\n",
    "Uses buffered data for training data and uses unbuffered data to return pseudoabsence by country. Buffered means the NUTS country shapefile was expanded outside of the country's borders to include islands, etc. Unbuffered means we used the NUTS shapefile as is when cutting out covariates. If we use Buffered data for pseudoabsence, it leads to double sampling of regions where buffered region data overlap between bordering countries. \n",
    "\n",
    "You can scale the number of pseudabsence points by a factor given by the **scaler** parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata_path = f'../../data/{modelname}/training/final/clustered/'\n",
    "region_code_list = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires all data to be present (in predictors data folder)\n",
    "cov_dict = path.cov_dict\n",
    "\n",
    "\n",
    "#Total Region area by predictor length\n",
    "region_area_dict = {}\n",
    "for region_code in region_list:\n",
    "    read_filename =  f'../../data/{modelname}/processed-predictor-parquets/clustered/{region_code}-predictors.parquet'\n",
    "    blah = pd.read_parquet(read_filename)\n",
    "    region_area_dict[region_code] = len(blah)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current method: batch by kmeans clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region_code in region_code_list:\n",
    "\n",
    "    ### Combine tick foci data with covariates by country, create pseudoabsence points. Save both files by country.\n",
    "    #read in processed buffered country predictor data\n",
    "    read_filename =  f'../../data/{modelname}/processed-predictor-parquets/clustered/{region_code}-predictors.parquet'\n",
    "    pred_df = pd.read_parquet(read_filename)\n",
    "    pred_df = gpd.GeoDataFrame(\n",
    "        pred_df, geometry=gpd.points_from_xy(pred_df['lon_env'], pred_df['lat_env']))\n",
    "    pred_df = pred_df.drop(['nuts_id','nuts_name','levl_code','cntr_code'],axis=1)\n",
    "    print(f'region - {region_code}')\n",
    "\n",
    "    #grab processed tick data\n",
    "    path_to_read_file=f'../../data/{modelname}/processed-master-database/'\n",
    "    print(path_to_read_file)\n",
    "    file_name_microfoci = 'cluster_df.csv'\n",
    "\n",
    "    tick_df = pd.read_csv(path_to_read_file+file_name_microfoci)\n",
    "    tick_df = tick_df[tick_df['region']==region_code]\n",
    "    tdf = tick_df[['presence', 'longitude','latitude','country_code','nuts_id','nuts_name','region']] #presence and obs_type seem to matchup\n",
    "    tgdf = gpd.GeoDataFrame(\n",
    "        tdf, geometry=gpd.points_from_xy(tdf['longitude'], tdf['latitude']))\n",
    "\n",
    "    #join tick and predictor data\n",
    "    train_df = tgdf.sjoin_nearest(pred_df, how='left').drop_duplicates(['latitude','longitude'])\n",
    "    training = train_df.drop(['geometry','index_right'],axis=1).reset_index(drop=True)\n",
    "\n",
    "    #output training data \n",
    "    output_region = f'region_{region_code}/'\n",
    "    training_filename = trainingdata_path + output_region + f'training_data/{region_code}-training-data.csv'\n",
    "    training.to_csv(training_filename)\n",
    "\n",
    "    ### pseudoabsence data section\n",
    "    print(f'Beginning pseudoabsence point df creation for region-{region_code}.')\n",
    "    #get subset of env data without tick data points\n",
    "    pseudoabsence = pred_df[~(pred_df['geometry'].isin(list(train_df['geometry'])))]\n",
    "\n",
    "    #sample proportion of pseudoabsence points relative to whole area under study\n",
    "    num_pseudo_pts = round(10000 * region_area_dict[region_code] / sum(list(region_area_dict.values())))\n",
    "    prop_value = num_pseudo_pts\n",
    "    print(f'Number of Pseudoabsence for region {region_code}: {num_pseudo_pts}')\n",
    "\n",
    "\n",
    "    #Sample covariates now using imported proportion\n",
    "    pseudoabsence['presence'] = 0\n",
    "    pseudoabsence_sample = pseudoabsence.sample((prop_value))\n",
    "        \n",
    "    for landcover in set(pred_df['cat']).difference(set(pseudoabsence_sample['cat'])):\n",
    "        pa_strat_sample = pseudoabsence[pseudoabsence['cat']==landcover].sample(1)\n",
    "        pseudoabsence_sample = pd.concat([pseudoabsence_sample, pa_strat_sample])\n",
    "\n",
    "\n",
    "    ### map nuts and admin localities to pseudoabsence points\n",
    "    print('Mapping Nuts locations to pseudoabsence points')\n",
    "    pseudoabsence_sample['region']=region_code\n",
    "    pseudoabsence_final = pr.nuts_join_regions(pseudoabsence_sample).drop(['levl_code'],axis=1)\n",
    "    pseudoabsence_final = pseudoabsence_final.rename({\n",
    "                'tg-grp-mean-days-above-5degc-monthly-ratio':'tg-grp-mean-days-above-5degC-monthly-ratio', \n",
    "                }, axis=1)  #rename the lowercase col that happened in the pr.nuts_join func)\n",
    "\n",
    "    # Sample now\n",
    "    pseudo_filename = trainingdata_path + output_region +f'pseudoabsence_data/{region_code}-pseudoabsence-data.csv'\n",
    "    pseudoabsence_final.to_csv(pseudo_filename)\n",
    "    print(f'{len(pseudoabsence_final)} Pseudoabsence points Saved for region-{region_code} at: {pseudo_filename}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
