{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "cwd=os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, cwd)\n",
    "import rasterio\n",
    "import folium\n",
    "import branca\n",
    "from folium.plugins import HeatMap\n",
    "import re\n",
    "import branca.colormap as cmp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import shapefile as shp\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import utils.processing as pr\n",
    "import utils.s3_utils as s3\n",
    "import utils.plot as pl\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from folium import plugins\n",
    "import config.paths as path\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import random\n",
    "import utils.modeling as ml\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "region_code = 2\n",
    "modelname = path.model_path\n",
    "\n",
    "trainingdata_path = f'../../data/{modelname}/training/final/clustered/region_{region_code}/'\n",
    "save_path=f'../../model/tree-models/final-tree-model-3/region-{region_code}/feature-selection/'\n",
    "\n",
    "drop_features = 'y'\n",
    "test_train = 'n' #Only set to 'y' after obtaining best training parameters using gridsearch.\n",
    "test_split = .2\n",
    "eval_method = 'precision'\n",
    "\n",
    "# Import data\n",
    "x = pd.read_csv(trainingdata_path + 'training_x/x.csv')\n",
    "x = x.drop(['Unnamed: 0'],axis=1)\n",
    "y = pd.read_csv(trainingdata_path + 'training_y/y.csv')\n",
    "y = y.drop(['Unnamed: 0'],axis=1)\n",
    "dummies_train = pd.read_csv(trainingdata_path + 'training_x/dummies.csv')\n",
    "dummies_train = dummies_train.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "x.shape\n",
    "\n",
    "#Remove pearson correlated features, while keeping the highest coeff feature in orig. model\n",
    "if drop_features == 'y':\n",
    "    drop_cols = path.xgboost_col_drop_dict[region_code]\n",
    "    x = x.drop(drop_cols, axis=1)\n",
    "    save_path=save_path\n",
    "    Path(f\"{save_path}\").mkdir(parents=True, exist_ok=True)\n",
    "else: \n",
    "    save_path=save_path\n",
    "    Path(f\"{save_path}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize variables\n",
    "nuts3_list = list(y.nuts_id.unique()) #will drop by 20% each run\n",
    "nuts3_total_districts = len(nuts3_list)\n",
    "random.seed(42)\n",
    "drop_descriptor_cols=['nuts_id','nuts_name','longitude','latitude']\n",
    "y_pred_list = []\n",
    "if test_train == 'y':\n",
    "    ### This section only runs if there is a saved clf model with best_params configured (produced in gridsearch section below)\n",
    "    ### This will error out if you do not have saved clf model params. Please set test_train config to 'y' once params are set. \n",
    "        \n",
    "    for i in range(0,int(1/test_split)):\n",
    "        #randomly select test_split % of all nuts_ids, drop them from nuts 3 set space for next run\n",
    "        #if-then clause makes sure we get the extra remaining nuts3 district from split\n",
    "        if i == max(range(0,int(1/test_split))):\n",
    "            test_nuts3_districts = nuts3_list\n",
    "        else:\n",
    "            test_nuts3_districts = random.sample(nuts3_list, int(nuts3_total_districts * test_split))\n",
    "        nuts3_list = [x for x in nuts3_list if x not in list(test_nuts3_districts)]\n",
    "        print(f'***NUTS3 IDs sampled for Run {i}: {test_nuts3_districts}***')\n",
    "        print(nuts3_list)\n",
    "\n",
    "        y_train = y[~y['nuts_id'].isin(test_nuts3_districts)]\n",
    "        x_train = x[~x['nuts_id'].isin(test_nuts3_districts)]\n",
    "\n",
    "        y_test = y[y['nuts_id'].isin(test_nuts3_districts)]\n",
    "        x_test = x[x['nuts_id'].isin(test_nuts3_districts)]\n",
    "\n",
    "        y_train = y_train.drop(drop_descriptor_cols, axis=1)\n",
    "        x_train = x_train.drop(drop_descriptor_cols, axis=1)\n",
    "\n",
    "        y_test = y_test.drop(['nuts_id','nuts_name'], axis=1)\n",
    "        x_test = x_test.drop(drop_descriptor_cols, axis=1)\n",
    "\n",
    "        final_param_dict = clf.best_params_    #for grid/randomsearchcv results\n",
    "        regxGB, importances = ml.xgboost_model(x_train,y_train, params=final_param_dict)\n",
    "\n",
    "\n",
    "        ### get roc-auc score by predicted using fitted model\n",
    "        y_test['pred'] = regxGB.predict_proba(x_test)[:, 1]\n",
    "        auroc_score = roc_auc_score(y_test['presence'], y_test['pred'])\n",
    "        y_pred_list.append(y_test)\n",
    "        \n",
    "        print(f'AUROC for Test Run {i}:{auroc_score}')\n",
    "        print(f'*** END OF RUN {i} ***')\n",
    "        print('')\n",
    "\n",
    "    print('***Cross-Validation Complete***\\n')\n",
    "    y_pred = pd.concat(y_pred_list)\n",
    "    y_pred = pr.remove_or_combine_duplicates(y_pred, strategy='aggregate', aggfunc='mean')\n",
    "\n",
    "    auroc_score = roc_auc_score(y_pred['presence'], y_pred['pred'])\n",
    "    print(f'***AUROC for Cross-Validation:{auroc_score}***')\n",
    "    ### output results\n",
    "\n",
    "    param_text = f'''\n",
    "    *** Region {region_code} Logistic Regression Results Summary\n",
    "    ROC-AUC score: {auroc_score}\n",
    "    features dropped: {drop_features}\n",
    "    dropped cols: {drop_cols}\n",
    "                        '''\n",
    "    param_text2 = str(regxGB.get_params())\n",
    "\n",
    "    with open(save_path+'AUROC-cv-summary.txt', \"w\") as text_file:\n",
    "        text_file.write(param_text)\n",
    "        text_file.write(param_text2)\n",
    "\n",
    "    print('***Dropping descriptor columns from x and y dfs***')\n",
    "    x = x.drop(drop_descriptor_cols, axis=1)\n",
    "    y = y.drop(drop_descriptor_cols, axis=1)\n",
    "\n",
    "if test_train == 'n':\n",
    "    x = x.drop(drop_descriptor_cols, axis=1)\n",
    "    y = y.drop(drop_descriptor_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation / Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### randomized search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_method = 'precision'\n",
    "\n",
    "# Initialize an XGBClassifier with a specific random seed for reproducibility\n",
    "xgbr = xgb.XGBClassifier(seed=30)\n",
    "\n",
    "# Define a dictionary of hyperparameters to tune\n",
    "# - max_depth: Maximum tree depth for base learners\n",
    "# - learning_rate: Boosting learning rate (xgb’s “eta”)\n",
    "# - n_estimators: Number of trees to fit\n",
    "# - colsample_bytree: Subsample ratio of columns when constructing each tree\n",
    "# - min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree (gamma)\n",
    "# - min_child_weight: Minimum sum of instance weight (hessian) needed in a child\n",
    "# - subsample: Subsample ratio of the training instances\n",
    "# - reg_lambda: L2 regularization term on weights (lambda)\n",
    "# - reg_alpha: L1 regularization term on weights (alpha)\n",
    "# - max_leaves: Maximum number of terminal nodes or leaves in a tree\n",
    "# - eval_metric: Evaluation metrics for validation data, a default metric will be assigned according to objective (roc_auc for binary classification)\n",
    "params = {'max_depth':list(np.arange(2, 8, step=1)) + [None],\n",
    "          'n_estimators':np.arange(200, 800, step=100),\n",
    "          'learning_rate': np.arange(.01, .02, step=.005),\n",
    "          'colsample_bytree': np.arange(0.3, .8, step=.1),\n",
    "          'min_child_weight': randint(0,1),\n",
    "          'min_split_loss': [0],\n",
    "          'subsample': np.arange(.4,1, step=.1),\n",
    "          'reg_lambda': np.arange(0, 1.6, step=.2),\n",
    "          'reg_alpha':  np.arange(0, 1.4, step=.2)\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV to find the best hyperparameters using cross-validation\n",
    "# - estimator: The model to be tuned\n",
    "# - param_grid: Dictionary with parameters names (str) as keys and lists of parameter settings to try as values\n",
    "# - verbose: Controls the verbosity: the higher, the more messages\n",
    "# - n_jobs: Number of jobs to run in parallel (-1 means using all processors)\n",
    "# - scoring: Strategy to evaluate the performance of the cross-validated model on the test set\n",
    "clf = RandomizedSearchCV(estimator=xgbr,\n",
    "                   param_distributions=params,\n",
    "                   n_iter=1500,\n",
    "                   verbose=2, \n",
    "                   n_jobs=-1, \n",
    "                   scoring=eval_method,\n",
    "                   cv=5)\n",
    "\n",
    "# Fit the GridSearchCV model\n",
    "clf.fit(x, y)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "\n",
    "# Printing best AUC score\n",
    "print(f'precision is: {clf.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(clf.best_params_)\n",
    "\n",
    "# Printing best AUC score\n",
    "print(f'precision is: {clf.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_method = 'precision'\n",
    "\n",
    "# Initialize an XGBClassifier with a specific random seed for reproducibility\n",
    "xgbr = xgb.XGBClassifier(seed=30)\n",
    "\n",
    "# Define a dictionary of hyperparameters to tune\n",
    "# - max_depth: Maximum tree depth for base learners\n",
    "# - learning_rate: Boosting learning rate (xgb’s “eta”)\n",
    "# - n_estimators: Number of trees to fit\n",
    "# - colsample_bytree: Subsample ratio of columns when constructing each tree\n",
    "# - min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree (gamma)\n",
    "# - min_child_weight: Minimum sum of instance weight (hessian) needed in a child\n",
    "# - subsample: Subsample ratio of the training instances\n",
    "# - reg_lambda: L2 regularization term on weights (lambda)\n",
    "# - reg_alpha: L1 regularization term on weights (alpha)\n",
    "# - max_leaves: Maximum number of terminal nodes or leaves in a tree\n",
    "# - eval_metric: Evaluation metrics for validation data, a default metric will be assigned according to objective (roc_auc for binary classification)\n",
    "params = {\n",
    "    'colsample_bytree': [.5,.6],\n",
    "    'learning_rate': [.012,.015],\n",
    "    'max_depth': [2,3,4],\n",
    "    'min_child_weight': [0],\n",
    "    'min_split_loss': [0],\n",
    "    'n_estimators': [600,700,800],\n",
    "    'reg_alpha':  [0,.1,.2],\n",
    "    'reg_lambda': [1,1.2],\n",
    "    'subsample': [.4,.5]\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize GridSearchCV to find the best hyperparameters using cross-validation\n",
    "# - estimator: The model to be tuned\n",
    "# - param_grid: Dictionary with parameters names (str) as keys and lists of parameter settings to try as values\n",
    "# - verbose: Controls the verbosity: the higher, the more messages\n",
    "# - n_jobs: Number of jobs to run in parallel (-1 means using all processors)\n",
    "# - scoring: Strategy to evaluate the performance of the cross-validated model on the test set\n",
    "clf = GridSearchCV(estimator=xgbr,\n",
    "                   param_grid=params,\n",
    "                   verbose=2, n_jobs=-1, scoring=eval_method,cv=5)\n",
    "\n",
    "# Fit the GridSearchCV model\n",
    "clf.fit(x, y)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "\n",
    "# Printing best AUC score\n",
    "print(f'precision is: {clf.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'precision is: {clf.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save params\n",
    "param_text = str(clf.best_params_)\n",
    "with open(save_path+'model_params_gridsearch.txt', \"w\") as text_file:\n",
    "   text_file.write(str(clf.best_score_))\n",
    "   text_file.write(param_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_param_dict = clf.best_params_    #for grid/randomsearchcv results\n",
    "param_text = str(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full model using all data points for training\n",
    "regxGB, importances= ml.xgboost_model(x,y, params=final_param_dict)\n",
    "\n",
    "# view and save feature importance results\n",
    "display(importances.head(15))\n",
    "save_name=f\"feature_importances.csv\"\n",
    "importances.to_csv(save_path+save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in-sample testing\n",
    "regxGB, importances= ml.xgboost_model(x,y, params=final_param_dict)\n",
    "y_pred = (regxGB.predict_proba(x)[:,1]).round(3)\n",
    "\n",
    "auroc_score = roc_auc_score(y['presence'], y_pred)\n",
    "print(f'***AUROC for Cross-Validation:{auroc_score}***')\n",
    "\n",
    "param_text = f'''\n",
    "*** Region {region_code} Logistic Regression Results Summary\n",
    "ROC-AUC score: {auroc_score}\n",
    "features dropped: {drop_features}\n",
    "dropped cols: {drop_cols}\n",
    "                    '''\n",
    "param_text2 = str(regxGB.get_params())\n",
    "with open(save_path+'AUROC-insample-summary.txt', \"w\") as text_file:\n",
    "    text_file.write(param_text)\n",
    "    text_file.write(param_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions on Cov Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test prediction on Covariates only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = f'../../data/{modelname}/processed-predictor-parquets/clustered/{region_code}-predictors.parquet'\n",
    "df = pd.read_parquet(fn, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Predictions\n",
    "#get predictors for all countries\n",
    "test_list = []\n",
    "sample_sizer = 5\n",
    "\n",
    "fn = f'../../data/{modelname}/processed-predictor-parquets/clustered/{region_code}-predictors.parquet'\n",
    "df = pd.read_parquet(fn, engine='pyarrow')\n",
    "samp_size = df.shape[0]/sample_sizer\n",
    "print(f'sample_size of {region_code} : {samp_size}')\n",
    "\n",
    "df = df.sample(int(np.round(samp_size)), random_state=42)\n",
    "test_list.append(df)\n",
    "\n",
    "test = pd.concat(test_list)\n",
    "\n",
    "del df, test_list\n",
    "\n",
    "\n",
    "test_df = test.drop(['geometry','landcover'],axis=1)\n",
    "test_df.columns = map(str.lower, test_df.columns)\n",
    "\n",
    "#get dummies for landcover \n",
    "dummies_test = pd.get_dummies(test_df['cat'])\n",
    "#add dummy categoricals to match dummies_train used in training\n",
    "for missing_env in set(dummies_train.columns).difference(set(dummies_test.columns)):\n",
    "    dummies_test[missing_env] = 0\n",
    "\n",
    "\n",
    "test_df2 = test_df.drop('cat',axis=1)\n",
    "test_df2 = pd.concat([test_df2, dummies_test], axis = 1).reset_index().drop('index',axis=1)\n",
    "\n",
    "#Match and Re-order columns to the same as was used in training dataframe x\n",
    "x_test = x.rename(columns = {'tg-grp-mean-days-above-5degC-monthly-ratio':'tg-grp-mean-days-above-5degc-monthly-ratio'})\n",
    "nolatlon = test_df2[x_test.columns]\n",
    "\n",
    "\n",
    "x_test = nolatlon\n",
    "x_test = x_test.rename(columns = {'tg-grp-mean-days-above-5degc-monthly-ratio':'tg-grp-mean-days-above-5degC-monthly-ratio'})\n",
    "\n",
    "print('Running prediction')\n",
    "test_pred = pd.DataFrame()\n",
    "test_pred['pred'] = (regxGB.predict_proba(x_test)[:,1]).round(3)\n",
    "test_pred['latitude'] = test_df2['lat_env']\n",
    "test_pred['longitude'] = test_df2['lon_env']\n",
    "\n",
    "print(test_pred.info())\n",
    "\n",
    "print('Outputting prediction data')\n",
    "print('removing duplicate columns')\n",
    "test_pred = pr.remove_or_combine_duplicates(test_pred, strategy='aggregate', aggfunc='mean')\n",
    "\n",
    "del  nolatlon, test_df2, test_df, dummies_test, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred.to_csv(save_path+f'test-predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist = test_pred.pred.hist(bins=100)\n",
    "fig = hist.figure\n",
    "fig.savefig(save_path+'distribution_histogram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Plot and Choropleth Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = ['DK','NO','SE','FI','AT','CH','CZ','DE','EE','FR', 'LT','LV','NL','PL','SK', 'IT','UK','HR','BE','SI','LU']\n",
    "\n",
    "m, ma = pl.plot_xgboost_maps(test_pred, country_codes,region=region_code, dot_sample=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name=f\"xgb-dotmap.html\"\n",
    "m.save(save_path+save_name)\n",
    "\n",
    "save_name=f\"xgb-choropleth.html\"\n",
    "ma.save(save_path+save_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_code = 0\n",
    "modelname = path.model_path\n",
    "\n",
    "trainingdata_path = f'../../data/{modelname}/training/final/clustered/region_{region_code}/'\n",
    "save_path=f'../../model/tree-models/final-tree-model-2/region-{region_code}/feature-selection/permutation_test/'\n",
    "\n",
    "drop_features = 'y'\n",
    "test_split = .2\n",
    "\n",
    "# Import data\n",
    "x = pd.read_csv(trainingdata_path + 'training_x/x.csv')\n",
    "x = x.drop(['Unnamed: 0'],axis=1)\n",
    "y = pd.read_csv(trainingdata_path + 'training_y/y.csv')\n",
    "y = y.drop(['Unnamed: 0'],axis=1)\n",
    "dummies_train = pd.read_csv(trainingdata_path + 'training_x/dummies.csv')\n",
    "dummies_train = dummies_train.drop(['Unnamed: 0'],axis=1)\n",
    "drop_descriptor_cols=['nuts_id','nuts_name','longitude','latitude']\n",
    "\n",
    "x.shape\n",
    "\n",
    "#Remove pearson correlated features, while keeping the highest coeff feature in orig. model\n",
    "if drop_features == 'y':\n",
    "    drop_cols = path.xgboost_col_drop_dict[region_code]\n",
    "    x = x.drop(drop_cols, axis=1)\n",
    "    save_path=save_path\n",
    "    Path(f\"{save_path}\").mkdir(parents=True, exist_ok=True)\n",
    "else: \n",
    "    save_path=save_path\n",
    "    Path(f\"{save_path}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_list = [col for col in x.columns if 'cat_' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC_degradation = {}\n",
    "base_AUROC = 0.728190967910594\n",
    "final_param_dict = {'objective': 'binary:logistic', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.3, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.012, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 9, 'max_leaves': None, 'min_child_weight': 0, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 250, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': 0.8, 'reg_lambda': 1.5, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.5, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'min_split_loss': 0}\n",
    "n = 10\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "for cov in set(x.columns).difference(set(landcover_list)).difference(set(drop_descriptor_cols)):\n",
    "    sum_auroc =0\n",
    "    print(f'***Run {cov}***')\n",
    "    for j in range(n):\n",
    "        x_shuffled = x.copy()\n",
    "        x_shuffled[cov] = np.random.permutation(x[cov].values)\n",
    "\n",
    "        ### begin cv\n",
    "        nuts3_list = list(y.nuts_id.unique()) #will drop by 20% each run\n",
    "        nuts3_total_districts = len(nuts3_list)\n",
    "        random.seed(42)\n",
    "        drop_descriptor_cols=['nuts_id','nuts_name','longitude','latitude']\n",
    "        y_pred_list = []\n",
    "        for i in range(0,int(1/test_split)):\n",
    "            #randomly select test_split % of all nuts_ids, drop them from nuts 3 set space for next run\n",
    "            #if-then clause makes sure we get the extra remaining nuts3 district from split\n",
    "            if i == max(range(0,int(1/test_split))):\n",
    "                test_nuts3_districts = nuts3_list\n",
    "            else:\n",
    "                test_nuts3_districts = random.sample(nuts3_list, int(nuts3_total_districts * test_split))\n",
    "            nuts3_list = [x for x in nuts3_list if x not in list(test_nuts3_districts)]\n",
    "\n",
    "            y_train = y[~y['nuts_id'].isin(test_nuts3_districts)]\n",
    "            x_train = x_shuffled[~x_shuffled['nuts_id'].isin(test_nuts3_districts)]\n",
    "\n",
    "            y_test = y[y['nuts_id'].isin(test_nuts3_districts)]\n",
    "            x_test = x[x_shuffled['nuts_id'].isin(test_nuts3_districts)]\n",
    "\n",
    "            y_train = y_train.drop(drop_descriptor_cols, axis=1)\n",
    "            x_train = x_train.drop(drop_descriptor_cols, axis=1)\n",
    "\n",
    "            y_test = y_test.drop(['nuts_id','nuts_name'], axis=1)\n",
    "            x_test = x_test.drop(drop_descriptor_cols, axis=1)\n",
    "\n",
    "            regxGB, importances = ml.xgboost_model(x_train,y_train, params=final_param_dict)\n",
    "\n",
    "            ### get roc-auc score by predicted using fitted model\n",
    "            y_test['pred'] = regxGB.predict_proba(x_test)[:, 1]\n",
    "            auroc_score = roc_auc_score(y_test['presence'], y_test['pred'])\n",
    "            y_pred_list.append(y_test)\n",
    "\n",
    "        y_pred = pd.concat(y_pred_list)\n",
    "        y_pred = pr.remove_or_combine_duplicates(y_pred, strategy='aggregate', aggfunc='mean')\n",
    "\n",
    "        auroc_score = roc_auc_score(y_pred['presence'], y_pred['pred'])\n",
    "        sum_auroc += auroc_score\n",
    "        \n",
    "    avg_auroc = sum_auroc / n #average of all n runs\n",
    "    print(f'***AUROC for n=10 {cov} CV:{avg_auroc}***')\n",
    "\n",
    "    AUROC_degradation[cov] = base_AUROC - avg_auroc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove landcover variables\n",
    "sum_auroc = 0\n",
    "for i in range(n):\n",
    "    x_shuffled = x.copy()\n",
    "    for cov in landcover_list:\n",
    "        x_shuffled[cov] = np.random.permutation(x[cov].values)\n",
    "        print(f'Shuffled values in {cov}')\n",
    "\n",
    "    ### begin cv\n",
    "    nuts3_list = list(y.nuts_id.unique()) #will drop by 20% each run\n",
    "    nuts3_total_districts = len(nuts3_list)\n",
    "    drop_descriptor_cols=['nuts_id','nuts_name','longitude','latitude']\n",
    "    y_pred_list = []\n",
    "    for i in range(0,int(1/test_split)):\n",
    "        #randomly select test_split % of all nuts_ids, drop them from nuts 3 set space for next run\n",
    "        #if-then clause makes sure we get the extra remaining nuts3 district from split\n",
    "        if i == max(range(0,int(1/test_split))):\n",
    "            test_nuts3_districts = nuts3_list\n",
    "        else:\n",
    "            test_nuts3_districts = random.sample(nuts3_list, int(nuts3_total_districts * test_split))\n",
    "        nuts3_list = [x for x in nuts3_list if x not in list(test_nuts3_districts)]\n",
    "\n",
    "        y_train = y[~y['nuts_id'].isin(test_nuts3_districts)]\n",
    "        x_train = x_shuffled[~x_shuffled['nuts_id'].isin(test_nuts3_districts)]\n",
    "\n",
    "        y_test = y[y['nuts_id'].isin(test_nuts3_districts)]\n",
    "        x_test = x[x_shuffled['nuts_id'].isin(test_nuts3_districts)]\n",
    "\n",
    "        y_train = y_train.drop(drop_descriptor_cols, axis=1)\n",
    "        x_train = x_train.drop(drop_descriptor_cols, axis=1)\n",
    "\n",
    "        y_test = y_test.drop(['nuts_id','nuts_name'], axis=1)\n",
    "        x_test = x_test.drop(drop_descriptor_cols, axis=1)\n",
    "\n",
    "        regxGB, importances = ml.xgboost_model(x_train,y_train, params=final_param_dict)\n",
    "\n",
    "        ### get roc-auc score by predicted using fitted model\n",
    "        y_test['pred'] = regxGB.predict_proba(x_test)[:, 1]\n",
    "        auroc_score = roc_auc_score(y_test['presence'], y_test['pred'])\n",
    "        y_pred_list.append(y_test)\n",
    "        \n",
    "\n",
    "    y_pred = pd.concat(y_pred_list)\n",
    "    y_pred = pr.remove_or_combine_duplicates(y_pred, strategy='aggregate', aggfunc='mean')\n",
    "\n",
    "    auroc_score = roc_auc_score(y_pred['presence'], y_pred['pred'])\n",
    "    print(f'***AUROC for Cross-Validation:{auroc_score}***')\n",
    "    sum_auroc += auroc_score\n",
    "\n",
    "avg_auroc = sum_auroc / n #average of all n runs\n",
    "print(f'***AUROC for n=10 landcover CV:{avg_auroc}***')\n",
    "\n",
    "AUROC_degradation['landcover'] = base_AUROC - avg_auroc\n",
    "\n",
    "\n",
    "print(AUROC_degradation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_degradation_df = pd.DataFrame(AUROC_degradation.items(), columns=['cov','loss']).sort_values('loss', ascending=False)\n",
    "auroc_degradation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_degradation_df.to_csv(save_path+'permutation-importance.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
